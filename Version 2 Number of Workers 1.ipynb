{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0a2f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/08 19:24:34 WARN TaskSetManager: Stage 244 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:24:38 WARN TaskSetManager: Stage 247 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:24:42 WARN TaskSetManager: Stage 250 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:24:45 WARN TaskSetManager: Stage 253 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:24:50 WARN TaskSetManager: Stage 254 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:24:53 WARN TaskSetManager: Stage 255 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:24:56 WARN TaskSetManager: Stage 256 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:25:05 WARN TaskSetManager: Stage 287 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:25:20 WARN TaskSetManager: Stage 288 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:25:23 WARN TaskSetManager: Stage 289 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:25:27 WARN TaskSetManager: Stage 290 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:25:38 WARN DAGScheduler: Broadcasting large task binary with size 1522.1 KiB\n",
      "24/02/08 19:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1007.3 KiB\n",
      "24/02/08 19:25:50 WARN DAGScheduler: Broadcasting large task binary with size 1683.3 KiB\n",
      "24/02/08 19:25:52 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/02/08 19:25:55 WARN TaskSetManager: Stage 407 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:00 WARN TaskSetManager: Stage 408 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:02 WARN TaskSetManager: Stage 409 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:04 WARN TaskSetManager: Stage 410 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:11 WARN TaskSetManager: Stage 441 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:18 WARN TaskSetManager: Stage 442 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:20 WARN TaskSetManager: Stage 443 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:23 WARN TaskSetManager: Stage 444 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:33 WARN DAGScheduler: Broadcasting large task binary with size 1532.9 KiB\n",
      "24/02/08 19:26:40 WARN DAGScheduler: Broadcasting large task binary with size 1000.3 KiB\n",
      "24/02/08 19:26:40 WARN DAGScheduler: Broadcasting large task binary with size 1658.7 KiB\n",
      "24/02/08 19:26:42 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/02/08 19:26:44 WARN TaskSetManager: Stage 561 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:49 WARN TaskSetManager: Stage 562 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:50 WARN TaskSetManager: Stage 563 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:52 WARN TaskSetManager: Stage 564 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:26:59 WARN TaskSetManager: Stage 595 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:08 WARN TaskSetManager: Stage 596 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:11 WARN TaskSetManager: Stage 597 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:15 WARN TaskSetManager: Stage 598 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:25 WARN DAGScheduler: Broadcasting large task binary with size 1525.5 KiB\n",
      "24/02/08 19:27:33 WARN DAGScheduler: Broadcasting large task binary with size 1012.9 KiB\n",
      "24/02/08 19:27:34 WARN DAGScheduler: Broadcasting large task binary with size 1704.6 KiB\n",
      "24/02/08 19:27:36 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/02/08 19:27:41 WARN TaskSetManager: Stage 715 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:47 WARN TaskSetManager: Stage 716 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:49 WARN TaskSetManager: Stage 717 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:27:51 WARN TaskSetManager: Stage 718 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:28:13 WARN DAGScheduler: Broadcasting large task binary with size 1017.5 KiB\n",
      "24/02/08 19:28:15 WARN DAGScheduler: Broadcasting large task binary with size 1742.4 KiB\n",
      "24/02/08 19:28:16 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/02/08 19:28:19 WARN TaskSetManager: Stage 762 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:28:25 WARN TaskSetManager: Stage 763 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:28:26 WARN TaskSetManager: Stage 764 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 19:28:28 WARN TaskSetManager: Stage 765 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059570631088793846\n",
      "235.5348732471466\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "def extract_year(input_string):\n",
    "    if len(input_string) < 4:\n",
    "        return None\n",
    "    return input_string[:4]\n",
    "\n",
    "def calculate_mape(predictions):\n",
    "    return predictions.withColumn(\"abs_diff\", spark_abs(col(\"Impact\") - col(\"prediction\"))) \\\n",
    "        .withColumn(\"mape\", spark_abs(col(\"Impact\") - col(\"prediction\")) / col(\"Impact\")) \\\n",
    "        .agg(mean(\"mape\")) \\\n",
    "        .collect()[0][0]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, size,split\n",
    "from pyspark.sql.functions import abs as spark_abs, mean\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import mean, abs as spark_abs, col\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "import bisect\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.executor.instances\",\"1\")  \n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")      \n",
    "\n",
    "# Create SparkSession with the configured parameters\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark in Jupyter\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "pandas_df = pd.read_csv('books_task.csv')\n",
    "pandas_df.drop('Unnamed: 0',inplace=True,axis=1)\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df = df.na.drop()\n",
    "\n",
    "\n",
    "#Author\n",
    "author_counts = df.groupBy(\"authors\").count()\n",
    "\n",
    "\n",
    "categorized_df_author = author_counts.withColumn(\"author_category\", \n",
    "                        when(col(\"count\") == 1, 1)\n",
    "                        .when(col(\"count\") == 2, 2)\n",
    "                        .otherwise(3))\n",
    "\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"author_category\"], outputCols=[\"author_category_encoded\"])\n",
    "encoder_model = encoder.fit(categorized_df_author)\n",
    "encoded_df_author = encoder_model.transform(categorized_df_author)\n",
    "\n",
    "\n",
    "df = df.join(encoded_df_author, on=\"authors\", how=\"left\")\n",
    "\n",
    "\n",
    "#Publisher\n",
    "publisher_counts = df.groupBy(\"publisher\").count()\n",
    "\n",
    "bins = [0, 5, 30, 150, 250, 1000, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "bucketizer = F.udf(lambda count: labels[min(bisect.bisect_right(bins, count), len(labels) - 1)], IntegerType())\n",
    "categorized_df_publisher = publisher_counts.withColumn(\"publisher_bucket\", bucketizer(\"count\"))\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"publisher_bucket\"], outputCols=[\"publisher_category_encoded\"])\n",
    "encoder_model = encoder.fit(categorized_df_publisher)\n",
    "encoded_df_publisher = encoder_model.transform(categorized_df_publisher)\n",
    "\n",
    "df = df.join(encoded_df_publisher.select(\"publisher\", \"publisher_category_encoded\"), on=\"publisher\", how=\"left\")\n",
    "\n",
    "#Category\n",
    "category_counts = df.groupBy(\"categories\").count()\n",
    "\n",
    "\n",
    "num_buckets = 5\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=num_buckets, inputCol=\"count\", outputCol=\"category_bucket\")\n",
    "\n",
    "discretized_df = discretizer.fit(category_counts).transform(category_counts)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"category_bucket\"], outputCols=[\"category_encoded\"])\n",
    "encoder_model = encoder.fit(discretized_df)\n",
    "encoded_df_categories = encoder_model.transform(discretized_df)\n",
    "\n",
    "# Join the encoded DataFrame back to the original DataFrame\n",
    "df = df.join(encoded_df_categories.select(\"categories\", \"category_encoded\"), on=\"categories\", how=\"left\")\n",
    "\n",
    "\n",
    "#Title\n",
    "\n",
    "df = df.withColumn(\"Title_Word_Count\", \n",
    "                   udf(lambda x: len(x.split()), IntegerType())(\"Title\"))\n",
    "\n",
    "df = df.withColumn(\"Title_Character_Count\", \n",
    "                   udf(lambda x: len(x), IntegerType())(\"Title\"))\n",
    "\n",
    "df = df.withColumn(\"Title_Avg_Word_Length\", \n",
    "                   udf(lambda x: sum(len(word) for word in x.split()) / len(x.split()), DoubleType())(\"Title\"))\n",
    "#Description\n",
    "\n",
    "df = df.withColumn(\"Description_Word_Count\", \n",
    "                   udf(lambda x: len(x.split()), IntegerType())(\"description\"))\n",
    "\n",
    "df = df.withColumn(\"Description_Character_Count\", \n",
    "                   udf(lambda x: len(x), IntegerType())(\"description\"))\n",
    "\n",
    "df = df.withColumn(\"Description_Avg_Word_Length\", \n",
    "                   udf(lambda x: sum(len(word) for word in x.split()) / len(x.split()), DoubleType())(\"description\"))\n",
    "#publishedDate\n",
    "\n",
    "extract_year_udf = udf(extract_year, StringType())\n",
    "df = df.withColumn(\"Year\", extract_year_udf(\"publishedDate\"))\n",
    "df = df.withColumn(\"Year\", col(\"Year\").cast(\"integer\"))\n",
    "\n",
    "#authors\n",
    "\n",
    "df = df.withColumn(\"authors\", split(col(\"authors\"), \",\"))\n",
    "df = df.withColumn(\"NumAuthors\", size(col(\"authors\")))\n",
    "\n",
    "\n",
    "\n",
    "Processed_Featured=['author_category_encoded','publisher_category_encoded','category_encoded','Title_Word_Count','Title_Character_Count','Title_Avg_Word_Length','Description_Word_Count','Description_Character_Count','Description_Avg_Word_Length','Year','NumAuthors','Impact']\n",
    "\n",
    "processed_df = df.select(Processed_Featured)\n",
    "\n",
    "\n",
    "# Model\n",
    "assembler = VectorAssembler(inputCols=['author_category_encoded','publisher_category_encoded','category_encoded',\n",
    "                                       'Title_Word_Count', 'Title_Character_Count', 'Title_Avg_Word_Length',\n",
    "                                       'Description_Word_Count', 'Description_Character_Count',\n",
    "                                       'Description_Avg_Word_Length', 'Year', 'NumAuthors'],\n",
    "                            outputCol='features', handleInvalid=\"skip\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Impact\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10,]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "# Create CrossValidator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"Impact\"),\n",
    "                          numFolds=3,\n",
    "                          collectSubModels=True)\n",
    "\n",
    "# Run cross-validation and choose the best set of parameters\n",
    "cvModel = crossval.fit(processed_df)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "# Get best model from CrossValidator\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(processed_df)\n",
    "\n",
    "# Calculate MAPE manually\n",
    "mape = calculate_mape(predictions)\n",
    "\n",
    "\n",
    "\n",
    "print(mape)\n",
    "\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed669b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
