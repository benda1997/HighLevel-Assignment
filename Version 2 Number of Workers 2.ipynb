{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0a2f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/08 21:24:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/08 21:24:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/08 21:24:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/02/08 21:24:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/02/08 21:24:50 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/02/08 21:25:02 WARN TaskSetManager: Stage 0 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:10 WARN TaskSetManager: Stage 3 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:15 WARN TaskSetManager: Stage 6 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:18 WARN TaskSetManager: Stage 9 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:20 WARN TaskSetManager: Stage 10 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:22 WARN TaskSetManager: Stage 11 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:23 WARN TaskSetManager: Stage 12 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:41 WARN TaskSetManager: Stage 43 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:44 WARN TaskSetManager: Stage 44 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:46 WARN TaskSetManager: Stage 45 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:25:48 WARN TaskSetManager: Stage 46 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:02 WARN DAGScheduler: Broadcasting large task binary with size 1523.4 KiB\n",
      "24/02/08 21:26:13 WARN DAGScheduler: Broadcasting large task binary with size 1010.0 KiB\n",
      "24/02/08 21:26:14 WARN DAGScheduler: Broadcasting large task binary with size 1692.7 KiB\n",
      "24/02/08 21:26:16 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/02/08 21:26:20 WARN TaskSetManager: Stage 163 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:22 WARN TaskSetManager: Stage 164 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:24 WARN TaskSetManager: Stage 165 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:25 WARN TaskSetManager: Stage 166 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:33 WARN TaskSetManager: Stage 197 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:37 WARN TaskSetManager: Stage 198 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:38 WARN TaskSetManager: Stage 199 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:40 WARN TaskSetManager: Stage 200 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:26:49 WARN DAGScheduler: Broadcasting large task binary with size 1544.5 KiB\n",
      "24/02/08 21:26:59 WARN DAGScheduler: Broadcasting large task binary with size 1020.3 KiB\n",
      "24/02/08 21:27:00 WARN DAGScheduler: Broadcasting large task binary with size 1727.6 KiB\n",
      "24/02/08 21:27:02 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/02/08 21:27:06 WARN TaskSetManager: Stage 317 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:08 WARN TaskSetManager: Stage 318 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:10 WARN TaskSetManager: Stage 319 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:11 WARN TaskSetManager: Stage 320 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:18 WARN TaskSetManager: Stage 351 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:22 WARN TaskSetManager: Stage 352 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:23 WARN TaskSetManager: Stage 353 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:25 WARN TaskSetManager: Stage 354 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:34 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "24/02/08 21:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1006.0 KiB\n",
      "24/02/08 21:27:43 WARN DAGScheduler: Broadcasting large task binary with size 1688.8 KiB\n",
      "24/02/08 21:27:45 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/02/08 21:27:49 WARN TaskSetManager: Stage 471 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:51 WARN TaskSetManager: Stage 472 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:52 WARN TaskSetManager: Stage 473 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:27:54 WARN TaskSetManager: Stage 474 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:28:24 WARN DAGScheduler: Broadcasting large task binary with size 1017.0 KiB\n",
      "24/02/08 21:28:26 WARN DAGScheduler: Broadcasting large task binary with size 1742.2 KiB\n",
      "24/02/08 21:28:29 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/02/08 21:28:32 WARN TaskSetManager: Stage 518 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:28:34 WARN TaskSetManager: Stage 519 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:28:36 WARN TaskSetManager: Stage 520 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/08 21:28:37 WARN TaskSetManager: Stage 521 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 529:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05950649145940839\n",
      "224.77991890907288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "def extract_year(input_string):\n",
    "    if len(input_string) < 4:\n",
    "        return None\n",
    "    return input_string[:4]\n",
    "\n",
    "def calculate_mape(predictions):\n",
    "    return predictions.withColumn(\"abs_diff\", spark_abs(col(\"Impact\") - col(\"prediction\"))) \\\n",
    "        .withColumn(\"mape\", spark_abs(col(\"Impact\") - col(\"prediction\")) / col(\"Impact\")) \\\n",
    "        .agg(mean(\"mape\")) \\\n",
    "        .collect()[0][0]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, size,split\n",
    "from pyspark.sql.functions import abs as spark_abs, mean\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import mean, abs as spark_abs, col\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "import bisect\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.executor.instances\",\"2\")  \n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")      \n",
    "\n",
    "# Create SparkSession with the configured parameters\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark in Jupyter\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "pandas_df = pd.read_csv('books_task.csv')\n",
    "pandas_df.drop('Unnamed: 0',inplace=True,axis=1)\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df = df.na.drop()\n",
    "\n",
    "\n",
    "#Author\n",
    "author_counts = df.groupBy(\"authors\").count()\n",
    "\n",
    "\n",
    "categorized_df_author = author_counts.withColumn(\"author_category\", \n",
    "                        when(col(\"count\") == 1, 1)\n",
    "                        .when(col(\"count\") == 2, 2)\n",
    "                        .otherwise(3))\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"author_category\"], outputCols=[\"author_category_encoded\"])\n",
    "encoder_model = encoder.fit(categorized_df_author)\n",
    "encoded_df_author = encoder_model.transform(categorized_df_author)\n",
    "\n",
    "\n",
    "df = df.join(encoded_df_author, on=\"authors\", how=\"left\")\n",
    "\n",
    "\n",
    "#Publisher\n",
    "publisher_counts = df.groupBy(\"publisher\").count()\n",
    "\n",
    "bins = [0, 5, 30, 150, 250, 1000, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "bucketizer = F.udf(lambda count: labels[min(bisect.bisect_right(bins, count), len(labels) - 1)], IntegerType())\n",
    "categorized_df_publisher = publisher_counts.withColumn(\"publisher_bucket\", bucketizer(\"count\"))\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"publisher_bucket\"], outputCols=[\"publisher_category_encoded\"])\n",
    "encoder_model = encoder.fit(categorized_df_publisher)\n",
    "encoded_df_publisher = encoder_model.transform(categorized_df_publisher)\n",
    "\n",
    "df = df.join(encoded_df_publisher.select(\"publisher\", \"publisher_category_encoded\"), on=\"publisher\", how=\"left\")\n",
    "\n",
    "#Category\n",
    "category_counts = df.groupBy(\"categories\").count()\n",
    "\n",
    "\n",
    "num_buckets = 5\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=num_buckets, inputCol=\"count\", outputCol=\"category_bucket\")\n",
    "discretized_df = discretizer.fit(category_counts).transform(category_counts)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"category_bucket\"], outputCols=[\"category_encoded\"])\n",
    "encoder_model = encoder.fit(discretized_df)\n",
    "encoded_df_categories = encoder_model.transform(discretized_df)\n",
    "\n",
    "df = df.join(encoded_df_categories.select(\"categories\", \"category_encoded\"), on=\"categories\", how=\"left\")\n",
    "\n",
    "\n",
    "#Title\n",
    "\n",
    "df = df.withColumn(\"Title_Word_Count\", \n",
    "                   udf(lambda x: len(x.split()), IntegerType())(\"Title\"))\n",
    "\n",
    "df = df.withColumn(\"Title_Character_Count\", \n",
    "                   udf(lambda x: len(x), IntegerType())(\"Title\"))\n",
    "\n",
    "df = df.withColumn(\"Title_Avg_Word_Length\", \n",
    "                   udf(lambda x: sum(len(word) for word in x.split()) / len(x.split()), DoubleType())(\"Title\"))\n",
    "#Description\n",
    "\n",
    "df = df.withColumn(\"Description_Word_Count\", \n",
    "                   udf(lambda x: len(x.split()), IntegerType())(\"description\"))\n",
    "\n",
    "df = df.withColumn(\"Description_Character_Count\", \n",
    "                   udf(lambda x: len(x), IntegerType())(\"description\"))\n",
    "\n",
    "df = df.withColumn(\"Description_Avg_Word_Length\", \n",
    "                   udf(lambda x: sum(len(word) for word in x.split()) / len(x.split()), DoubleType())(\"description\"))\n",
    "#publishedDate\n",
    "\n",
    "extract_year_udf = udf(extract_year, StringType())\n",
    "df = df.withColumn(\"Year\", extract_year_udf(\"publishedDate\"))\n",
    "df = df.withColumn(\"Year\", col(\"Year\").cast(\"integer\"))\n",
    "\n",
    "#authors\n",
    "\n",
    "df = df.withColumn(\"authors\", split(col(\"authors\"), \",\"))\n",
    "df = df.withColumn(\"NumAuthors\", size(col(\"authors\")))\n",
    "\n",
    "\n",
    "\n",
    "Processed_Featured=['author_category_encoded','publisher_category_encoded','category_encoded','Title_Word_Count','Title_Character_Count','Title_Avg_Word_Length','Description_Word_Count','Description_Character_Count','Description_Avg_Word_Length','Year','NumAuthors','Impact']\n",
    "\n",
    "processed_df = df.select(Processed_Featured)\n",
    "\n",
    "\n",
    "# Model\n",
    "assembler = VectorAssembler(inputCols=['author_category_encoded','publisher_category_encoded','category_encoded',\n",
    "                                       'Title_Word_Count', 'Title_Character_Count', 'Title_Avg_Word_Length',\n",
    "                                       'Description_Word_Count', 'Description_Character_Count',\n",
    "                                       'Description_Avg_Word_Length', 'Year', 'NumAuthors'],\n",
    "                            outputCol='features', handleInvalid=\"skip\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Impact\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10,]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "# Create CrossValidator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"Impact\"),\n",
    "                          numFolds=3,\n",
    "                          collectSubModels=True)\n",
    "\n",
    "# Run cross-validation and choose the best set of parameters\n",
    "cvModel = crossval.fit(processed_df)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "# Get best model from CrossValidator\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(processed_df)\n",
    "\n",
    "# Calculate MAPE manually\n",
    "mape = calculate_mape(predictions)\n",
    "\n",
    "\n",
    "\n",
    "print(mape)\n",
    "\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed669b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
