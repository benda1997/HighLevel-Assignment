{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0a2f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/09 00:26:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/09 00:26:33 WARN TaskSetManager: Stage 0 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:39 WARN TaskSetManager: Stage 3 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:42 WARN TaskSetManager: Stage 6 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:44 WARN TaskSetManager: Stage 9 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:45 WARN TaskSetManager: Stage 10 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:46 WARN TaskSetManager: Stage 11 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:47 WARN TaskSetManager: Stage 12 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:56 WARN TaskSetManager: Stage 43 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:57 WARN TaskSetManager: Stage 44 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:58 WARN TaskSetManager: Stage 45 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:26:59 WARN TaskSetManager: Stage 46 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1526.4 KiB\n",
      "24/02/09 00:27:12 WARN DAGScheduler: Broadcasting large task binary with size 1020.3 KiB\n",
      "24/02/09 00:27:13 WARN DAGScheduler: Broadcasting large task binary with size 1734.7 KiB\n",
      "24/02/09 00:27:14 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/02/09 00:27:16 WARN TaskSetManager: Stage 163 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:17 WARN TaskSetManager: Stage 164 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:18 WARN TaskSetManager: Stage 165 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:19 WARN TaskSetManager: Stage 166 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:25 WARN TaskSetManager: Stage 208 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:26 WARN TaskSetManager: Stage 209 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:27 WARN TaskSetManager: Stage 210 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:28 WARN TaskSetManager: Stage 211 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:34 WARN DAGScheduler: Broadcasting large task binary with size 1496.3 KiB\n",
      "24/02/09 00:27:40 WARN DAGScheduler: Broadcasting large task binary with size 1668.1 KiB\n",
      "24/02/09 00:27:41 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/02/09 00:27:43 WARN TaskSetManager: Stage 362 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:44 WARN TaskSetManager: Stage 363 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:45 WARN TaskSetManager: Stage 364 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:46 WARN TaskSetManager: Stage 365 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:53 WARN TaskSetManager: Stage 407 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:54 WARN TaskSetManager: Stage 408 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:55 WARN TaskSetManager: Stage 409 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:27:57 WARN TaskSetManager: Stage 410 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:03 WARN DAGScheduler: Broadcasting large task binary with size 1486.2 KiB\n",
      "24/02/09 00:28:09 WARN DAGScheduler: Broadcasting large task binary with size 1006.8 KiB\n",
      "24/02/09 00:28:10 WARN DAGScheduler: Broadcasting large task binary with size 1708.1 KiB\n",
      "24/02/09 00:28:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/02/09 00:28:13 WARN TaskSetManager: Stage 561 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:14 WARN TaskSetManager: Stage 562 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:15 WARN TaskSetManager: Stage 563 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:16 WARN TaskSetManager: Stage 564 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:33 WARN DAGScheduler: Broadcasting large task binary with size 1008.0 KiB\n",
      "24/02/09 00:28:34 WARN DAGScheduler: Broadcasting large task binary with size 1726.1 KiB\n",
      "24/02/09 00:28:35 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/02/09 00:28:37 WARN TaskSetManager: Stage 608 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:38 WARN TaskSetManager: Stage 609 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:39 WARN TaskSetManager: Stage 610 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/02/09 00:28:40 WARN TaskSetManager: Stage 611 contains a task of very large size (12602 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05953079471966947\n",
      "137.92987084388733\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "def extract_year(input_string):\n",
    "    if len(input_string) < 4:\n",
    "        return None\n",
    "    return input_string[:4]\n",
    "\n",
    "def calculate_mape(predictions):\n",
    "    return predictions.withColumn(\"abs_diff\", spark_abs(col(\"Impact\") - col(\"prediction\"))) \\\n",
    "        .withColumn(\"mape\", spark_abs(col(\"Impact\") - col(\"prediction\")) / col(\"Impact\")) \\\n",
    "        .agg(mean(\"mape\")) \\\n",
    "        .collect()[0][0]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, size,split\n",
    "from pyspark.sql.functions import abs as spark_abs, mean\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import mean, abs as spark_abs, col\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "import bisect\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.executor.instances\",\"4\")  \n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")      \n",
    "\n",
    "# Create SparkSession with the configured parameters\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark in Jupyter\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "pandas_df = pd.read_csv('books_task.csv')\n",
    "pandas_df.drop('Unnamed: 0',inplace=True,axis=1)\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df = df.na.drop()\n",
    "\n",
    "\n",
    "#Author\n",
    "author_counts = df.groupBy(\"authors\").count()\n",
    "\n",
    "\n",
    "categorized_df_author = author_counts.withColumn(\"author_category\", \n",
    "                        when(col(\"count\") == 1, 1)\n",
    "                        .when(col(\"count\") == 2, 2)\n",
    "                        .otherwise(3))\n",
    "\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"author_category\"], outputCols=[\"author_category_encoded\"])\n",
    "encoder_model = encoder.fit(categorized_df_author)\n",
    "encoded_df_author = encoder_model.transform(categorized_df_author)\n",
    "\n",
    "\n",
    "df = df.join(encoded_df_author, on=\"authors\", how=\"left\")\n",
    "\n",
    "\n",
    "#Publisher\n",
    "publisher_counts = df.groupBy(\"publisher\").count()\n",
    "\n",
    "bins = [0, 5, 30, 150, 250, 1000, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "bucketizer = F.udf(lambda count: labels[min(bisect.bisect_right(bins, count), len(labels) - 1)], IntegerType())\n",
    "categorized_df_publisher = publisher_counts.withColumn(\"publisher_bucket\", bucketizer(\"count\"))\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"publisher_bucket\"], outputCols=[\"publisher_category_encoded\"])\n",
    "encoder_model = encoder.fit(categorized_df_publisher)\n",
    "encoded_df_publisher = encoder_model.transform(categorized_df_publisher)\n",
    "\n",
    "df = df.join(encoded_df_publisher.select(\"publisher\", \"publisher_category_encoded\"), on=\"publisher\", how=\"left\")\n",
    "\n",
    "#Category\n",
    "category_counts = df.groupBy(\"categories\").count()\n",
    "\n",
    "\n",
    "num_buckets = 5\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=num_buckets, inputCol=\"count\", outputCol=\"category_bucket\")\n",
    "\n",
    "discretized_df = discretizer.fit(category_counts).transform(category_counts)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"category_bucket\"], outputCols=[\"category_encoded\"])\n",
    "encoder_model = encoder.fit(discretized_df)\n",
    "encoded_df_categories = encoder_model.transform(discretized_df)\n",
    "\n",
    "# Join the encoded DataFrame back to the original DataFrame\n",
    "df = df.join(encoded_df_categories.select(\"categories\", \"category_encoded\"), on=\"categories\", how=\"left\")\n",
    "\n",
    "\n",
    "#Title\n",
    "\n",
    "df = df.withColumn(\"Title_Word_Count\", \n",
    "                   udf(lambda x: len(x.split()), IntegerType())(\"Title\"))\n",
    "\n",
    "df = df.withColumn(\"Title_Character_Count\", \n",
    "                   udf(lambda x: len(x), IntegerType())(\"Title\"))\n",
    "\n",
    "df = df.withColumn(\"Title_Avg_Word_Length\", \n",
    "                   udf(lambda x: sum(len(word) for word in x.split()) / len(x.split()), DoubleType())(\"Title\"))\n",
    "#Description\n",
    "\n",
    "df = df.withColumn(\"Description_Word_Count\", \n",
    "                   udf(lambda x: len(x.split()), IntegerType())(\"description\"))\n",
    "\n",
    "df = df.withColumn(\"Description_Character_Count\", \n",
    "                   udf(lambda x: len(x), IntegerType())(\"description\"))\n",
    "\n",
    "df = df.withColumn(\"Description_Avg_Word_Length\", \n",
    "                   udf(lambda x: sum(len(word) for word in x.split()) / len(x.split()), DoubleType())(\"description\"))\n",
    "#publishedDate\n",
    "\n",
    "extract_year_udf = udf(extract_year, StringType())\n",
    "df = df.withColumn(\"Year\", extract_year_udf(\"publishedDate\"))\n",
    "df = df.withColumn(\"Year\", col(\"Year\").cast(\"integer\"))\n",
    "\n",
    "#authors\n",
    "\n",
    "df = df.withColumn(\"authors\", split(col(\"authors\"), \",\"))\n",
    "df = df.withColumn(\"NumAuthors\", size(col(\"authors\")))\n",
    "\n",
    "\n",
    "\n",
    "Processed_Featured=['author_category_encoded','publisher_category_encoded','category_encoded','Title_Word_Count','Title_Character_Count','Title_Avg_Word_Length','Description_Word_Count','Description_Character_Count','Description_Avg_Word_Length','Year','NumAuthors','Impact']\n",
    "\n",
    "processed_df = df.select(Processed_Featured)\n",
    "\n",
    "\n",
    "# Model\n",
    "assembler = VectorAssembler(inputCols=['author_category_encoded','publisher_category_encoded','category_encoded',\n",
    "                                       'Title_Word_Count', 'Title_Character_Count', 'Title_Avg_Word_Length',\n",
    "                                       'Description_Word_Count', 'Description_Character_Count',\n",
    "                                       'Description_Avg_Word_Length', 'Year', 'NumAuthors'],\n",
    "                            outputCol='features', handleInvalid=\"skip\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Impact\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10,]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "# Create CrossValidator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"Impact\"),\n",
    "                          numFolds=3,\n",
    "                          collectSubModels=True)\n",
    "\n",
    "# Run cross-validation and choose the best set of parameters\n",
    "cvModel = crossval.fit(processed_df)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "# Get best model from CrossValidator\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(processed_df)\n",
    "\n",
    "# Calculate MAPE manually\n",
    "mape = calculate_mape(predictions)\n",
    "\n",
    "\n",
    "\n",
    "print(mape)\n",
    "\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed669b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
